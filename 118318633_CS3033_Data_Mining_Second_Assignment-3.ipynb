{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "118318633 CS3033 - Data Mining - Second Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS3033/CS6405 - Data Mining - Second Assignment\n",
        "\n",
        "### Submission\n",
        "\n",
        "This assignment is **due on 06/04/22 at 23:59**. You should submit a single .ipnyb file with your python code and analysis electronically via Canvas.\n",
        "Please note that this assignment will account for 25 Marks of your module grade.\n",
        "\n",
        "### Declaration\n",
        "\n",
        "By submitting this assignment. I agree to the following:\n",
        "\n",
        "<font color=\"red\">“I have read and understand the UCC academic policy on plagiarism, and agree to the requirements set out thereby in relation to plagiarism and referencing. I confirm that I have referenced and acknowledged properly all sources used in the preparation of this assignment.\n",
        "I declare that this assignment is entirely my own work based on my personal study. I further declare that I have not engaged the services of another to either assist me in, or complete this assignment”</font>\n",
        "\n",
        "### Objective\n",
        "\n",
        "The Boolean satisfiability (SAT) problem consists in determining whether a Boolean formula F is satisfiable or not. F is represented by a pair (X, C), where X is a set of Boolean variables and C is a set of clauses in Conjunctive Normal Form (CNF). Each clause is a disjunction of literals (a variable or its negation). This problem is one of the most widely studied combinatorial problems in computer science. It is the classic NP-complete problem. Over the past number of decades, a significant amount of research work has focused on solving SAT problems with both complete and incomplete solvers.\n",
        "\n",
        "Recent advances in supervised learning have provided powerful techniques for classifying problems. In this project, we see the SAT problem as a classification problem. Given a Boolean formula (represented by a vector of features), we are asked to predict if it is satisfiable or not.\n",
        "\n",
        "In this project, we represent SAT problems with a vector of 327 features with general information about the problem, e.g., number of variables, number of clauses, fraction of horn clauses in the problem, etc. There is no need to understand the features to be able to complete the assignment.\n",
        "\n",
        "The dataset is available at:\n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/dm_assignment2/sat_dataset_train.csv\n",
        "\n",
        "This is original unpublished data."
      ],
      "metadata": {
        "id": "8WfrCFmLHxYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "Oav9G1WSJ1nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://github.com/andvise/DataAnalyticsDatasets/blob/6d5738101d173b97c565f143f945dedb9c42a400/dm_assignment2/sat_dataset_train.csv?raw=true\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "DE0kM0QsJ1En",
        "outputId": "aa056004-35d2-4c59-de61-52bb4ef2084c"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     c   v  clauses_vars_ratio  vars_clauses_ratio  vcg_var_mean  \\\n",
              "0  420  10           42.000000            0.023810      0.600000   \n",
              "1  230  20           11.500000            0.086957      0.137826   \n",
              "2  240  16           15.000000            0.066667      0.300000   \n",
              "3  424  30           14.133333            0.070755      0.226415   \n",
              "4  162  19            8.526316            0.117284      0.139701   \n",
              "\n",
              "   vcg_var_coeff  vcg_var_min  vcg_var_max  vcg_var_entropy  vcg_clause_mean  \\\n",
              "0       0.000000     0.600000     0.600000         0.000000         0.600000   \n",
              "1       0.089281     0.117391     0.160870         2.180946         0.137826   \n",
              "2       0.000000     0.300000     0.300000         0.000000         0.300000   \n",
              "3       0.485913     0.056604     0.452830         2.220088         0.226415   \n",
              "4       0.121821     0.111111     0.185185         1.940843         0.139701   \n",
              "\n",
              "   ...  rwh_0_max    rwh_1_mean  rwh_1_coeff     rwh_1_min     rwh_1_max  \\\n",
              "0  ...    78750.0      0.000008          0.0  7.875000e-06      0.000008   \n",
              "1  ...  6646875.0  17433.722184          1.0  2.981244e-12  34867.444369   \n",
              "2  ...   500000.0   1525.878932          0.0  1.525879e+03   1525.878932   \n",
              "3  ...    87500.0      0.000122          1.0  6.535723e-14      0.000245   \n",
              "4  ...  5859400.0  16591.494310          1.0  6.912726e-42  33182.988621   \n",
              "\n",
              "     rwh_2_mean  rwh_2_coeff     rwh_2_min     rwh_2_max  target  \n",
              "0  2.385082e-21          0.0  2.385082e-21  2.385082e-21       1  \n",
              "1  1.727721e+04          1.0  1.358551e-53  3.455442e+04       0  \n",
              "2  1.525879e+03          0.0  1.525879e+03  1.525879e+03       1  \n",
              "3  8.218628e-07          1.0  1.499676e-61  1.643726e-06       0  \n",
              "4  1.665903e+04          1.0  0.000000e+00  3.331807e+04       1  \n",
              "\n",
              "[5 rows x 328 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11eeb118-5033-4b67-a1b5-2540552d58b0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>v</th>\n",
              "      <th>clauses_vars_ratio</th>\n",
              "      <th>vars_clauses_ratio</th>\n",
              "      <th>vcg_var_mean</th>\n",
              "      <th>vcg_var_coeff</th>\n",
              "      <th>vcg_var_min</th>\n",
              "      <th>vcg_var_max</th>\n",
              "      <th>vcg_var_entropy</th>\n",
              "      <th>vcg_clause_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>rwh_0_max</th>\n",
              "      <th>rwh_1_mean</th>\n",
              "      <th>rwh_1_coeff</th>\n",
              "      <th>rwh_1_min</th>\n",
              "      <th>rwh_1_max</th>\n",
              "      <th>rwh_2_mean</th>\n",
              "      <th>rwh_2_coeff</th>\n",
              "      <th>rwh_2_min</th>\n",
              "      <th>rwh_2_max</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>420</td>\n",
              "      <td>10</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>0.023810</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>...</td>\n",
              "      <td>78750.0</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.875000e-06</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>2.385082e-21</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.385082e-21</td>\n",
              "      <td>2.385082e-21</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>230</td>\n",
              "      <td>20</td>\n",
              "      <td>11.500000</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.137826</td>\n",
              "      <td>0.089281</td>\n",
              "      <td>0.117391</td>\n",
              "      <td>0.160870</td>\n",
              "      <td>2.180946</td>\n",
              "      <td>0.137826</td>\n",
              "      <td>...</td>\n",
              "      <td>6646875.0</td>\n",
              "      <td>17433.722184</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.981244e-12</td>\n",
              "      <td>34867.444369</td>\n",
              "      <td>1.727721e+04</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.358551e-53</td>\n",
              "      <td>3.455442e+04</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>240</td>\n",
              "      <td>16</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>...</td>\n",
              "      <td>500000.0</td>\n",
              "      <td>1525.878932</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.525879e+03</td>\n",
              "      <td>1525.878932</td>\n",
              "      <td>1.525879e+03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.525879e+03</td>\n",
              "      <td>1.525879e+03</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>424</td>\n",
              "      <td>30</td>\n",
              "      <td>14.133333</td>\n",
              "      <td>0.070755</td>\n",
              "      <td>0.226415</td>\n",
              "      <td>0.485913</td>\n",
              "      <td>0.056604</td>\n",
              "      <td>0.452830</td>\n",
              "      <td>2.220088</td>\n",
              "      <td>0.226415</td>\n",
              "      <td>...</td>\n",
              "      <td>87500.0</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.535723e-14</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>8.218628e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.499676e-61</td>\n",
              "      <td>1.643726e-06</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>162</td>\n",
              "      <td>19</td>\n",
              "      <td>8.526316</td>\n",
              "      <td>0.117284</td>\n",
              "      <td>0.139701</td>\n",
              "      <td>0.121821</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.185185</td>\n",
              "      <td>1.940843</td>\n",
              "      <td>0.139701</td>\n",
              "      <td>...</td>\n",
              "      <td>5859400.0</td>\n",
              "      <td>16591.494310</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.912726e-42</td>\n",
              "      <td>33182.988621</td>\n",
              "      <td>1.665903e+04</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.331807e+04</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 328 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11eeb118-5033-4b67-a1b5-2540552d58b0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11eeb118-5033-4b67-a1b5-2540552d58b0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11eeb118-5033-4b67-a1b5-2540552d58b0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOqpQTS-K8EN",
        "outputId": "39bbad62-61cb-41a4-b036-fa6f09dc7178"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "c                       int64\n",
              "v                       int64\n",
              "clauses_vars_ratio    float64\n",
              "vars_clauses_ratio    float64\n",
              "vcg_var_mean          float64\n",
              "                       ...   \n",
              "rwh_2_mean            float64\n",
              "rwh_2_coeff           float64\n",
              "rwh_2_min             float64\n",
              "rwh_2_max             float64\n",
              "target                  int64\n",
              "Length: 328, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing infinity values with nan values and then filling nan values with 0 so the data can be used for tests"
      ],
      "metadata": {
        "id": "D4RcKMnbIYkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "df['target'].value_counts()\n",
        "#Change infinity value to nan\n",
        "df = df.replace([np.inf, -np.inf], np.nan)\n",
        "#Filling Nan with 0\n",
        "df = df.fillna(0)\n",
        "df.shape\n",
        "print(df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8MCvTYTKw4Q",
        "outputId": "8f574c09-ae98-4c96-c58c-379ced2217b2"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        c    v  clauses_vars_ratio  vars_clauses_ratio  vcg_var_mean  \\\n",
            "0     420   10           42.000000            0.023810      0.600000   \n",
            "1     230   20           11.500000            0.086957      0.137826   \n",
            "2     240   16           15.000000            0.066667      0.300000   \n",
            "3     424   30           14.133333            0.070755      0.226415   \n",
            "4     162   19            8.526316            0.117284      0.139701   \n",
            "...   ...  ...                 ...                 ...           ...   \n",
            "1924  910   50           18.200000            0.054945      0.045055   \n",
            "1925  440   30           14.666667            0.068182      0.229091   \n",
            "1926  372   28           13.285714            0.075269      0.103111   \n",
            "1927  821  181            4.535912            0.220463      0.019105   \n",
            "1928  775   90            8.611111            0.116129      0.033118   \n",
            "\n",
            "      vcg_var_coeff  vcg_var_min  vcg_var_max  vcg_var_entropy  \\\n",
            "0          0.000000     0.600000     0.600000         0.000000   \n",
            "1          0.089281     0.117391     0.160870         2.180946   \n",
            "2          0.000000     0.300000     0.300000         0.000000   \n",
            "3          0.485913     0.056604     0.452830         2.220088   \n",
            "4          0.121821     0.111111     0.185185         1.940843   \n",
            "...             ...          ...          ...              ...   \n",
            "1924       0.201127     0.024176     0.067033         3.204742   \n",
            "1925       0.463952     0.054545     0.436364         2.239837   \n",
            "1926       0.081916     0.080645     0.112903         2.008196   \n",
            "1927       0.033859     0.017052     0.019488         0.696675   \n",
            "1928       0.018366     0.032258     0.033548         0.636514   \n",
            "\n",
            "      vcg_clause_mean  ...  rwh_0_max    rwh_1_mean  rwh_1_coeff  \\\n",
            "0            0.600000  ...    78750.0  7.875000e-06     0.000000   \n",
            "1            0.137826  ...  6646875.0  1.743372e+04     1.000000   \n",
            "2            0.300000  ...   500000.0  1.525879e+03     0.000000   \n",
            "3            0.226415  ...    87500.0  1.223252e-04     1.000000   \n",
            "4            0.139701  ...  5859400.0  1.659149e+04     1.000000   \n",
            "...               ...  ...        ...           ...          ...   \n",
            "1924         0.045055  ...  7031250.0  5.937497e+03     1.000000   \n",
            "1925         0.229091  ...    29000.0  9.189624e-08     0.999173   \n",
            "1926         0.103111  ...  6640650.0  1.154713e+04     1.000000   \n",
            "1927         0.019105  ...  3515625.0  1.073111e+02     1.000000   \n",
            "1928         0.033118  ...  1640625.0  3.489123e+01     0.999429   \n",
            "\n",
            "          rwh_1_min     rwh_1_max    rwh_2_mean  rwh_2_coeff     rwh_2_min  \\\n",
            "0      7.875000e-06  7.875000e-06  2.385082e-21      0.00000  2.385082e-21   \n",
            "1      2.981244e-12  3.486744e+04  1.727721e+04      1.00000  1.358551e-53   \n",
            "2      1.525879e+03  1.525879e+03  1.525879e+03      0.00000  1.525879e+03   \n",
            "3      6.535723e-14  2.446504e-04  8.218628e-07      1.00000  1.499676e-61   \n",
            "4      6.912726e-42  3.318299e+04  1.665903e+04      1.00000  0.000000e+00   \n",
            "...             ...           ...           ...          ...           ...   \n",
            "1924  2.625169e-175  1.187499e+04  5.759320e+03      1.00000  0.000000e+00   \n",
            "1925   7.595547e-11  1.837165e-07  9.778364e-24      1.00000  3.978383e-33   \n",
            "1926   1.363938e-42  2.309426e+04  1.136608e+04      1.00000  0.000000e+00   \n",
            "1927   0.000000e+00  2.146221e+02  1.073113e+02      1.00000  0.000000e+00   \n",
            "1928   1.992578e-02  6.976254e+01  4.823067e+01      0.99921  3.808224e-02   \n",
            "\n",
            "         rwh_2_max  target  \n",
            "0     2.385082e-21       1  \n",
            "1     3.455442e+04       0  \n",
            "2     1.525879e+03       1  \n",
            "3     1.643726e-06       0  \n",
            "4     3.331807e+04       1  \n",
            "...            ...     ...  \n",
            "1924  1.151864e+04       1  \n",
            "1925  1.955673e-23       0  \n",
            "1926  2.273216e+04       0  \n",
            "1927  2.146226e+02       1  \n",
            "1928  9.642326e+01       0  \n",
            "\n",
            "[1929 rows x 328 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seperating the target from the rest of the features"
      ],
      "metadata": {
        "id": "Ce5Etwr-Ijxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,327].values\n",
        "\n"
      ],
      "metadata": {
        "id": "IEjqR2xLKjec"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks\n",
        "\n",
        "## Basic models and evaluation (5 Marks)\n",
        "\n",
        "Using Scikit-learn, train and evaluate K-NN and decision tree classifiers using 70% of the dataset from training and 30% for testing. For this part of the project, we are not interested in optimising the parameters; we just want to get an idea of the dataset. Compare the results of both classifiers."
      ],
      "metadata": {
        "id": "MTvkBPQvITf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN**\n",
        "Imported train_test_split to split the data into training and test for the features and label data\n",
        "Scaling the data using a MinMaxSacler\n",
        "Fitting the data to the features train data\n",
        "transforming the data\n",
        "performing knn with k value of 5 as a standard hyperparameter to start with\n",
        "predicting on test data\n",
        "calculating accuracy\n",
        "and printing\n",
        "\n",
        "**Decision Tree**\n",
        "Imported Decision tree classifier \n",
        "Set random state to 0\n",
        "Fitting the data to the features and label train data\n",
        "testing on test set\n",
        "calculating accuracy\n",
        "and printing"
      ],
      "metadata": {
        "id": "LHO4b6W9IrLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing Sklearn to train the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30)\n",
        "#Scaling the data \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#Training the data using .fit to predict \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "K = KNeighborsClassifier(n_neighbors=5)\n",
        "K.fit(X_train, y_train)\n",
        "prediction = K.predict(X_test)\n",
        "print(prediction)\n",
        "accuracy = sum(prediction==y_test)/y_test.shape[0]\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "#Decision tree\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier(random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "print(predictions)\n",
        "acc = sum(predictions==y_test)/y_test.shape[0]\n",
        "print(acc)\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zl0VXO0YH1nG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0497e4ec-8dfe-41e5-dcd6-26be00489d63"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0]\n",
            "Accuracy: 0.8981001727115717\n",
            "[0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0\n",
            " 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0]\n",
            "0.9792746113989638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy of the desision tree is much higher than the accuracy of the KNN. This is most likely due to the number of data points and the data is overscaled so the accuracy for the decision tree is extremly high - therefore I will use the Decision tree for the robust evalutaion as it is performing better"
      ],
      "metadata": {
        "id": "TJWPFfY-fWs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robust evaluation (10 Marks)\n",
        "\n",
        "In this section, we are interested in more rigorous techniques by implementing more sophisticated methods, for instance:\n",
        "* Hold-out and cross-validation.\n",
        "* Hyper-parameter tuning.\n",
        "* Feature reduction.\n",
        "* Feature normalisation.\n",
        "\n",
        "Your report should provide concrete information of your reasoning; everything should be well-explained.\n",
        "\n",
        "Do not get stressed if the things you try do not improve the accuracy. The key to geting good marks is to show that you evaluated different methods and that you correctly selected the configuration."
      ],
      "metadata": {
        "id": "zADpr0f8IcGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hold-out and cross-validation**\n",
        "\n",
        "Performed Hold out and cross validation for the KNN and Decision tree. \n",
        "The accuracy for the KNN stayed the same, but the accuracy for the decision tree improved by 1%. This showed that for this data the decision tree is a better model of evaluation and has a high accuracy. This method allows for multiple train and test data set tests, therefore it is more likely to have a higher accuracy"
      ],
      "metadata": {
        "id": "oykyy52yKyhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hold out and cross validation for the KNN\n",
        "from sklearn import model_selection\n",
        "scores = model_selection.cross_val_score(K, X_train, y_train, cv=10)\n",
        "print(scores)\n",
        "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
        "# Hold out and cross validation for the Decision Tree\n",
        "from sklearn import model_selection\n",
        "scores = model_selection.cross_val_score(classifier, X_train, y_train, cv=10)\n",
        "print(scores)\n",
        "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
      ],
      "metadata": {
        "id": "tvBZH6ilInsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54717c4c-ac9a-487e-d6dd-8c7a709802b2"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.92592593 0.84444444 0.87407407 0.92592593 0.86666667 0.91851852\n",
            " 0.88148148 0.83703704 0.83703704 0.88148148]\n",
            "0.88 accuracy with a standard deviation of 0.03\n",
            "[0.98518519 0.96296296 0.98518519 0.97777778 0.97777778 0.98518519\n",
            " 0.98518519 0.97037037 0.97777778 0.97777778]\n",
            "0.98 accuracy with a standard deviation of 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyper-parameter tuning - Decision Tree.**\n",
        "\n",
        "Set up a function for the decision tree to allow fro muliple random sets, test hyper parameters. I tested 7 different random values, the hyper parameter 13 had the highest accuracy with 98.4, this is an improvement on the original decision tree with a random state of 0 but is the same as the hold out cross validation. "
      ],
      "metadata": {
        "id": "dsIndQ8qL5i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_tree(x,y,i):\n",
        "  classifier = DecisionTreeClassifier(random_state=i) \n",
        "  classifier.fit(X_train, y_train)\n",
        "  predictions = classifier.predict(X_test)\n",
        "  acc = sum(predictions==y_test)/y_test.shape[0]\n",
        "  return acc\n",
        "DecisionTree1 = decision_tree(X_train, y_train,1)\n",
        "DecisionTree3 = decision_tree(X_train, y_train,3)\n",
        "DecisionTree5 = decision_tree(X_train, y_train,5)\n",
        "DecisionTree63 = decision_tree(X_train, y_train,63)\n",
        "DecisionTree103 = decision_tree(X_train, y_train,103)\n",
        "DecisionTree45 = decision_tree(X_train, y_train,45)\n",
        "DecisionTree13 = decision_tree(X_train, y_train,13)\n",
        "print(\"Accuracy when the hyperparameter is 1 for the descision tree : \",(DecisionTree1))\n",
        "print(\"Accuracy when the hyperparameter is 3 for the descision tree : \",DecisionTree3)\n",
        "print(\"Accuracy when the hyperparameter is 5 for the descision tree : \",DecisionTree5)\n",
        "print(\"Accuracy when the hyperparameter is 63 for the descision tree : \",DecisionTree63)\n",
        "print(\"Accuracy when the hyperparameter is 103 for the descision tree : \",DecisionTree103)\n",
        "print(\"Accuracy when the hyperparameter is 45 for the descision tree : \",DecisionTree45)\n",
        "print(\"Accuracy when the hyperparameter is 13 for the descision tree : \",DecisionTree13)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML-cPSTgLliU",
        "outputId": "46a0fc86-fb47-42f0-8ca1-5dc15fc07ad9"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy when the hyperparameter is 1 for the descision tree :  0.9792746113989638\n",
            "Accuracy when the hyperparameter is 3 for the descision tree :  0.9775474956822107\n",
            "Accuracy when the hyperparameter is 5 for the descision tree :  0.9810017271157168\n",
            "Accuracy when the hyperparameter is 63 for the descision tree :  0.9792746113989638\n",
            "Accuracy when the hyperparameter is 103 for the descision tree :  0.9810017271157168\n",
            "Accuracy when the hyperparameter is 45 for the descision tree :  0.9879101899827288\n",
            "Accuracy when the hyperparameter is 13 for the descision tree :  0.9810017271157168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up a pipeline to allow me to test PCA and the min max scaler and the standard scaler, to perform feature reduction and normalisation. PCA is a form of feature reduction, it allows for the main parts of the data to be used to change the basis of the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Yauv-qB5NBPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature reduction - selecting a subset of the original features using PCA\n",
        "# Use pipeline to pass in different components and understand which is better \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "classifier = Pipeline([(\"scaler\", MinMaxScaler()), \n",
        "    (\"pca\", PCA(n_components=3)),\n",
        "    (\"predictor\", DecisionTreeClassifier())])\n",
        "classifier.fit(X_train, y_train)\n",
        "classifier.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AItNacjLmAk",
        "outputId": "afade219-60d9-4175-989b-2c7bca88128b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
              "       0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
              "       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
              "       1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
              "       1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
              "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
              "       0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
              "       1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "       0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
              "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 1, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used pipeline to try 5 different hyperparameters when the data is scaled using the min max scaler and PCA. The PCA's accuracy is not as high as the other methods.\n",
        "Peformed a grid search on the data, fitted it to the X train and y train set\n",
        "Printed the best parameter and the best score as well as the accuracy. Then used the best parameter on the test set and computed the accuracy. I then repeated these steps for the StandardScaler.\n",
        "The accuracy for the minmax scaler on the test data was higher than the standard scaler. However, both of the accuracys are lower than the hyperparameter tuning above. I used the same hyperpapramters as above to allow for fair testing\n"
      ],
      "metadata": {
        "id": "BFZ2SaBkNqHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a dictionary of hyperparameters for the pipeline with the classifier\n",
        "# Feature reduction - selecting a subset of the original features using PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "classifier = Pipeline([(\"scaler\", MinMaxScaler()), \n",
        "     (\"pca\", PCA()),\n",
        "     (\"predictor\", DecisionTreeClassifier())])\n",
        "param_grid = {\"pca__n_components\": [1,3,5,63,103,45,13],\n",
        "                   \"predictor__criterion\": [\"gini\", \"entropy\"]}\n",
        " # Create a grid search variable that will find the best hyperparameter values based on validation error\n",
        "dt_gs = GridSearchCV(classifier, param_grid, scoring=\"accuracy\")\n",
        "dt_gs.fit(X_train, y_train)\n",
        "dt_gs.best_params_, dt_gs.best_score_\n",
        "\n",
        "best = classifier.set_params(**dt_gs.best_params_) \n",
        "print(best)\n",
        " # Fit the pipeline to the train data and print accuracy for the test set\n",
        "classifier.fit(X_train, y_train)\n",
        "acc = accuracy_score(y_test, classifier.predict(X_test))\n",
        "print(acc)\n",
        " # Repesting the same steps but using the best parameneter, entropy\n",
        "classifier = Pipeline([(\"scaler\", MinMaxScaler()), \n",
        "     (\"pca\", PCA()),\n",
        "     (\"predictor\", DecisionTreeClassifier())])\n",
        "yparam_grid = {\"pca__n_components\": [1,3,5,63,103,45,13],\n",
        "                   \"predictor__criterion\": [\"entropy\"]}\n",
        "ydt_gs = GridSearchCV(classifier, yparam_grid, scoring=\"accuracy\")\n",
        "ydt_gs.fit(X_train, y_train)\n",
        "ydt_gs.best_params_, ydt_gs.best_score_\n",
        "\n",
        "best = classifier.set_params(**ydt_gs.best_params_) \n",
        "print(best)\n",
        "classifier.fit(X_train, y_train)\n",
        "acc1 = accuracy_score(y_test, classifier.predict(X_test))\n",
        "print(acc1)\n",
        "\n",
        "\n",
        "# Feature normalisation \n",
        "# Use min max scaler and the standard scaler and see which is better \n",
        " # Repeating the above steps but changing the scaler\n",
        "classifier1 = Pipeline([(\"scaler\", StandardScaler()), \n",
        "     (\"pca\", PCA()),\n",
        "     (\"predictor\", DecisionTreeClassifier())])\n",
        "xparam_grid = {\"pca__n_components\": [1,3,5,63,103,45,13],\n",
        "                   \"predictor__criterion\": [\"gini\", \"entropy\"]}\n",
        " # Create the grid search object which will find the best hyperparameter values based on validation error\n",
        "xdt_gs = GridSearchCV(classifier1, xparam_grid, scoring=\"accuracy\")\n",
        "xdt_gs.fit(X_train, y_train)\n",
        "\n",
        "best1 = classifier1.set_params(**xdt_gs.best_params_) \n",
        "print(best1)\n",
        "classifier1.fit(X_train, y_train)\n",
        "acc2 = accuracy_score(y_test, classifier1.predict(X_test))\n",
        "print(acc2)\n",
        "\n",
        "classifier2 = Pipeline([(\"scaler\", MinMaxScaler()), \n",
        "     (\"pca\", PCA()),\n",
        "     (\"predictor\", DecisionTreeClassifier())])\n",
        "nparam_grid = {\"pca__n_components\": [1,3,5,63,103,45,13],\n",
        "                   \"predictor__criterion\": [\"entropy\"]}\n",
        "ndt_gs = GridSearchCV(classifier2, nparam_grid, scoring=\"accuracy\")\n",
        "ndt_gs.fit(X_train, y_train)\n",
        "\n",
        "best1 = classifier2.set_params(**ndt_gs.best_params_) \n",
        "classifier2.fit(X_train, y_train)\n",
        "acc3 = accuracy_score(y_test, classifier2.predict(X_test))\n",
        "print(acc3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYLnaiyacr2Z",
        "outputId": "68e5033b-5e42-412d-d68d-f37d22d17d10"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline(steps=[('scaler', MinMaxScaler()), ('pca', PCA(n_components=63)),\n",
            "                ('predictor', DecisionTreeClassifier(criterion='entropy'))])\n",
            "0.8618307426597582\n",
            "Pipeline(steps=[('scaler', MinMaxScaler()), ('pca', PCA(n_components=63)),\n",
            "                ('predictor', DecisionTreeClassifier(criterion='entropy'))])\n",
            "0.8652849740932642\n",
            "Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=45)),\n",
            "                ('predictor', DecisionTreeClassifier(criterion='entropy'))])\n",
            "0.9084628670120898\n",
            "0.8687392055267703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New classifier (10 Marks)\n",
        "\n",
        "Replicate the previous task for a classifier that we did not cover in class. So different than K-NN and decision trees. Briefly describe your choice.\n",
        "Try to create the best model for the given dataset.\n",
        "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset:\n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/dm_assignment2/sat_dataset_test.csv\n",
        "\n",
        "This link currently contains a sample of the training set. The real test set will be released after the submission. I should be able to run the code cell independently, load all the libraries you need as well."
      ],
      "metadata": {
        "id": "FYoMg0EZIrNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classifier I chose is MLP, Neural networks, it works by creating several layers of inputs and outputs, they are connected. It calculated the gradient of the error against the weight of the model. MLP had an accuracy of 95% which is lower then the decision tree which had 98% accuracy\n",
        "\n",
        "Resources used for MLP:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "https://panjeh.medium.com/scikit-learn-hyperparameter-optimization-for-mlpclassifier-4d670413042b"
      ],
      "metadata": {
        "id": "dbCDgyexuTpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MLP Neural network models\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "mlpclassifier = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
        "mlpclassifier.predict(X_test)\n",
        "mlpclassifier.predict(X_test)\n",
        "mlpclassifier.score(X_test, y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "QRJXrY2hI32F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df01452-d0b8-491f-cc56-100eba2c3b72"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9499136442141624"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performed Hold out and cross validation for MLP, this improved the accuracy by 1%. However it is still lower than when this method was used on the decision tree, that gave 99% accuracy."
      ],
      "metadata": {
        "id": "KL4TcrYQv4_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hold out and cross validation for MLP\n",
        "from sklearn import model_selection\n",
        "mlpscore = model_selection.cross_val_score(mlpclassifier, X_train, y_train, cv=10)\n",
        "print(mlpscore)\n",
        "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (mlpscore.mean(), mlpscore.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqGnTp1cu6rC",
        "outputId": "a04fc5f6-5506-44ac-86d4-cb71bac9a71d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.94814815 0.99259259 0.96296296 0.94814815 0.95555556 0.97037037\n",
            " 0.92592593 0.95555556 0.94074074 0.97037037]\n",
            "0.96 accuracy with a standard deviation of 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyper-parameter tuning - MLP.**\n",
        "Performed a grid search for MLP to determine the best hyperparameters, they are 'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'adam'. Then with those results, calculated the mean and standard deviation. Using these values I creatd a for loop to iterate through all hyperparamenters and calculated the mean standard deviation for them all. Put it against the test set and printed out a gris showing the precision, recall, f1-score and accuracy of the model. The accuracy computed was 95%, this is the same as the hold out cross validation model and less than the accuracy gotten for the decision tree which was 98%"
      ],
      "metadata": {
        "id": "hssEDLHfwPX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "parameters = {\n",
        "    'hidden_layer_sizes': [(10,),(20,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "mlpclf = GridSearchCV(mlpclassifier, parameters, n_jobs=-1, cv=5)\n",
        "mlpclf.fit(X_train, y_train) \n",
        "print('Best parameters found:\\n', mlpclf.best_params_)\n",
        "mlpmeans = mlpclf.cv_results_['mean_test_score']\n",
        "mlpstds = mlpclf.cv_results_['std_test_score']\n",
        "for mlpmean, mlpstd, mlpparams in zip(mlpmeans, mlpstds, mlpclf.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\" % (mlpmean, mlpstd * 2, mlpparams))\n",
        "\n",
        "y_true, y_pred = y_test , mlpclf.predict(X_test)\n",
        "\n",
        "print('Results on the test set:')\n",
        "print(classification_report(y_true, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E02EySLMwJ-5",
        "outputId": "dee5d6f5-b5e5-43eb-c2e9-2d5173158dab"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:\n",
            " {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.887 (+/-0.048) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.950 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.887 (+/-0.048) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.950 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.885 (+/-0.049) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.959 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.885 (+/-0.049) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.959 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.886 (+/-0.047) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.949 (+/-0.022) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.886 (+/-0.047) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.949 (+/-0.022) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.884 (+/-0.049) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.959 (+/-0.021) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.884 (+/-0.049) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.959 (+/-0.021) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.884 (+/-0.032) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.947 (+/-0.013) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.884 (+/-0.032) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.947 (+/-0.013) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.882 (+/-0.049) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.954 (+/-0.017) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.882 (+/-0.049) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.954 (+/-0.017) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.884 (+/-0.032) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.946 (+/-0.010) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.884 (+/-0.032) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.946 (+/-0.010) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.881 (+/-0.050) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.953 (+/-0.019) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.881 (+/-0.050) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.953 (+/-0.019) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "Results on the test set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.94       276\n",
            "           1       0.97      0.92      0.95       303\n",
            "\n",
            "    accuracy                           0.94       579\n",
            "   macro avg       0.94      0.95      0.94       579\n",
            "weighted avg       0.95      0.94      0.94       579\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Feature reduction - selecting a subset of the original features using PCA\n",
        "Use pipeline to pass in different components and understand which is better. When accuracy is computed against the tes set, the accuracy is only 95, which is lower than the accuracy computed for the desicion tree.  \n",
        "The accuracy does not change a significant amount when changing the scaler (data normalisation)"
      ],
      "metadata": {
        "id": "zsjIkvhY20sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA and standard scaler with pipeline for mlp \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "pca = PCA()\n",
        "scaler = StandardScaler()\n",
        "logistic = LogisticRegression(max_iter=1000, tol=0.1)\n",
        "pipe = Pipeline(steps=[(\"scaler\", scaler), (\"pca\", pca), (\"logistic\", logistic)])\n",
        "param_grid = {\n",
        "    \"pca__n_components\": [1,3,5,63,103,45,13],\n",
        "    \"logistic__C\": np.logspace(-4, 4, 4),\n",
        "}\n",
        "mlp_gs = GridSearchCV(pipe, param_grid, n_jobs=2)\n",
        "mlp_gs.fit(X_train, y_train)\n",
        "print(\"Best parameter %0.3f):\" % mlp_gs.best_score_)\n",
        "print(mlp_gs.best_params_)\n",
        "mlpaccpca = accuracy_score(y_train, mlp_gs.predict(X_train))\n",
        "print(mlpaccpca)\n",
        "mlpaccpcat = accuracy_score(y_test, mlp_gs.predict(X_test))\n",
        "print(mlpaccpcat)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VACt-LhC23vq",
        "outputId": "9023a39c-03b7-4c28-bac9-2175c81eaa7f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameter 0.976):\n",
            "{'logistic__C': 21.54434690031882, 'pca__n_components': 63}\n",
            "0.9955555555555555\n",
            "0.9585492227979274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA and minmax with pipeline for mlp \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "pca1 = PCA()\n",
        "scaler1 = MinMaxScaler()\n",
        "logistic1 = LogisticRegression(max_iter=1000, tol=0.1)\n",
        "pipe1 = Pipeline(steps=[(\"scaler\", scaler1), (\"pca\", pca1), (\"logistic\", logistic1)])\n",
        "param_grid1 = {\n",
        "    \"pca__n_components\": [1,3,5,63,103,45,13],\n",
        "    \"logistic__C\": np.logspace(-4, 4, 4),\n",
        "}\n",
        "mlp_gs1 = GridSearchCV(pipe1, param_grid1, n_jobs=2)\n",
        "mlp_gs1.fit(X_train, y_train)\n",
        "print(\"Best parameter %0.3f):\" % mlp_gs1.best_score_)\n",
        "print(mlp_gs1.best_params_)\n",
        "mlpaccpca1 = accuracy_score(y_train, mlp_gs1.predict(X_train))\n",
        "print(mlpaccpca1)\n",
        "mlpaccpcat1 = accuracy_score(y_test, mlp_gs1.predict(X_test))\n",
        "print(mlpaccpcat1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn_lDg_83eSN",
        "outputId": "2d5ba998-ed2d-4ca0-c7bb-c6fc740e40de"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameter 0.960):\n",
            "{'logistic__C': 10000.0, 'pca__n_components': 103}\n",
            "1.0\n",
            "0.9585492227979274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best Model: Decision tree with hyperparameter of 13, has the highest accuracy with 98.7, originally holdout cross validation had the highest accuracy with 99%."
      ],
      "metadata": {
        "id": "7LiUCUvz2V54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_tree(x,y,i):\n",
        "  classifier = DecisionTreeClassifier(random_state=i) \n",
        "  classifier.fit(X_train, y_train)\n",
        "  predictions = classifier.predict(X_test)\n",
        "  acc = sum(predictions==y_test)/y_test.shape[0]\n",
        "  return acc\n",
        "DecisionTree13 = decision_tree(X_train, y_train,13)\n",
        "print(\"Accuracy when the hyperparameter is 13 for the descision tree : \",DecisionTree13)"
      ],
      "metadata": {
        "id": "NoeDaNCN2R43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">FOR GRADING ONLY</font>\n",
        "\n",
        "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset: \n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/dm_assignment2/sat_dataset_test.csv"
      ],
      "metadata": {
        "id": "Q01BjiiCJTR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I could not get the githib push to work - the bestModel file would not push to my github"
      ],
      "metadata": {
        "id": "PNiF39gt9B69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Laurenigoe/Datamining\n",
        "%cd Datamining\n",
        "!git init\n",
        "!git config --global user.email \"laurenigoe1@gmail.com\"\n",
        "!git config --global user.name \"Laurenigoe\"\n",
        "\n",
        "import pickle\n",
        "pickle.dump(classifier,  open( \"model.pkl\", \"wb\" ))\n",
        "clss = pickle.load(open(\"model.pkl\", \"rb\"))\n",
        "clss\n",
        "clss.predict(X_test)\n",
        "\n",
        "from joblib import dump, load\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "# INSERT YOUR MODEL'S URL\n",
        "mLink = 'https://github.com/Laurenigoe/Datamining/blob/main/model.pkl?raw=true'\n",
        "mfile = BytesIO(requests.get(mLink).content)\n",
        "model = load(mfile)\n",
        "model\n"
      ],
      "metadata": {
        "id": "IWx4lyuQI929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "155e9af5-f4f7-413a-8343-a8e4ba4f30ac"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Datamining'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 3 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (9/9), done.\n",
            "/content/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Assignment2/Assignment2/Assignment2/Assignment2/Assignment2/Assignment2/Assignment2/Assignment2/Assignment2/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining\n",
            "Reinitialized existing Git repository in /content/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Assignment2/Assignment2/Assignment2/Assignment2/Assignment2/Assignment2/Assignment2/Assignment2/Assignment2/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/Datamining/.git/\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-8c0da6a45ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmLink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://github.com/Laurenigoe/Datamining/blob/main/model.pkl?raw=true'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmLink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
            "\u001b[0;32m/usr/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 10"
          ]
        }
      ]
    }
  ]
}